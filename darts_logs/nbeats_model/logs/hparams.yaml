activation: ReLU
batch_norm: false
dropout: 0.0
expansion_coefficient_dim: 5
generic_architecture: true
input_chunk_length: 24
input_dim: 1
layer_widths:
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
- 256
likelihood: null
lr_scheduler_cls: null
lr_scheduler_kwargs: null
nr_params: 1
num_blocks: 1
num_layers: 4
num_stacks: 30
optimizer_cls: !!python/name:torch.optim.adam.Adam ''
optimizer_kwargs: null
output_chunk_length: 12
output_chunk_shift: 0
output_dim: 1
train_sample_shape:
- !!python/tuple
  - 24
  - 1
- null
- null
- !!python/tuple
  - 12
  - 1
trend_polynomial_degree: 2
use_reversible_instance_norm: false

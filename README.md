# Time Series Forecasting Tool for

A comprehensive time series forecasting platform that enables consultants to upload, clean, analyze, visualize, and forecast data with minimal technical knowledge. The platform follows clean code principles, SOLID design, and provides an intuitive interface with powerful backend capabilities.

## Project Status

![In Development](Images/istockphoto-527660774-612x612.jpg)

**Current Phase**: Prototype development with focus on data pipeline implementation

- âœ… Basic Streamlit interface implemented
- âœ… Development environment with UV configured
- âœ… Type safety foundation with MyPy
- ðŸ”„ In Progress: Robust data processing pipeline
- ðŸ”„ In Progress: Code quality improvements

_Image source: [iStock](https://media.istockphoto.com/id/527660774/vector/under-construction-industrial-sign.jpg?s=612x612&w=0&k=20&c=3U2TR5u_Drl4B5HBRc13wHD32nZe38UhlB6hzkj93U0=)_

## Key Features

1. **Data Upload & Validation**

   - Multi-format support (CSV, Excel, JSON)
   - Intelligent error detection and reporting
   - Schema inference with Pydantic validation
   - Robust data pipeline for preprocessing

2. **Data Exploration & Cleaning**

   - Automated column type detection
   - Interactive data preview and profiling
   - Outlier detection and handling
   - NaN handling strategies
   - Time series-specific transformations

3. **Model Training & Selection**

   - Multiple time series models:
     - N-BEATS (Neural Basis Expansion Analysis for Interpretable Time Series Forecasting)
     - TIDE (Temporal Importance-Guided Denoising Encoder)
     - Prophet (Facebook's time series forecasting tool)
     - TimeMixer (A novel approach combining temporal convolutions and attention)
     - TFT (Temporal Fusion Transformers)
     - ARIMA (Autoregressive Integrated Moving Average)
   - Automated model selection based on data characteristics
   - Custom model configuration
   - Experiment tracking with MLflow/Neptune

4. **Forecasting & Analysis**

   - Interactive forecast visualization
   - Confidence intervals and uncertainty quantification
   - Feature importance analysis
   - What-if scenario testing

5. **Reporting & Export**
   - Customizable PDF reports
   - Excel/CSV exports with metadata
   - Interactive dashboard sharing

## Technical Stack

### Frontend

- **Current**: Streamlit (for rapid prototyping)
- **Planned**: React with TypeScript, Tailwind CSS, Redux Toolkit, React Query
- **Visualization**: Plotly.js, D3.js

### Backend

- **API Framework**: FastAPI with Pydantic
- **Forecasting Libraries**: Darts, Prophet
- **Data Processing**: Polars, NumPy, scikit-learn
- **Testing**: Pytest, Hypothesis
- **Documentation**: OpenAPI/Swagger
- **Type Checking**: MyPy with strict typing
- **Data Version Control**: DVC (planned)
- **Configuration**: Dynaconf (planned)

### Infrastructure

- **Containerization**: Docker, Docker Compose
- **CI/CD**: GitHub Actions
- **Monitoring & Tracking**:
  - System Monitoring: Prometheus, Grafana (planned)
  - ML Experiment Tracking: MLflow/Neptune (planned)

## Development Roadmap

### Prototype Phase (Current)

- Streamlit-based interface
- Core data processing pipeline
- Basic model integration
- Data validation and cleaning

### MVP Phase (Next)

- Enhanced time series processing
- Comprehensive model evaluation
- Improved Streamlit UI
- Basic reporting capabilities
- Documentation and testing

### Production Phase (Future)

- React frontend migration
- Advanced modeling features
- Comprehensive monitoring
- Deployment automation
- Enterprise security features

## Getting Started

### Prerequisites

- Python 3.11+
- Git

### Development Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/timeseries.git
cd timeseries

# Set up Python environment with UV
python -m venv .timeseries
source .timeseries/bin/activate  # On Windows: .timeseries\Scripts\activate
pip install uv
uv pip install -r requirements.txt

# Run the Streamlit app (for development)
cd backend/app
PYTHONPATH=../.. streamlit run streamlit.py  # On Windows: set PYTHONPATH=..\.. && streamlit run streamlit.py
```

### Running Tests

```bash
# From project root
pytest tests/
```

## Project Structure

```
timeseries/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/                     # API endpoints
â”‚   â”‚   â”œâ”€â”€ routes/              # API route definitions
â”‚   â”‚   â””â”€â”€ schemas/             # API request/response schemas
â”‚   â”œâ”€â”€ app/                     # Main application code
â”‚   â”œâ”€â”€ application/             # Application layer
â”‚   â”‚   â””â”€â”€ services/            # Application-specific services
â”‚   â”œâ”€â”€ Cleaning/                # Data cleaning modules
â”‚   â”‚   â””â”€â”€ models/              # Cleaning models and algorithms
â”‚   â”œâ”€â”€ config/                  # Configuration settings
â”‚   â”œâ”€â”€ core/                    # Core functionality
â”‚   â”‚   â”œâ”€â”€ config/              # Core configuration
â”‚   â”‚   â””â”€â”€ interfaces/          # Core interfaces/protocols
â”‚   â”œâ”€â”€ data/                    # Data processing modules
â”‚   â”œâ”€â”€ domain/                  # Domain layer (business logic)
â”‚   â”‚   â”œâ”€â”€ models/              # Domain models
â”‚   â”‚   â”‚   â”œâ”€â”€ boosting/        # Gradient boosting models
â”‚   â”‚   â”‚   â”œâ”€â”€ deep_learning/   # Neural network models
â”‚   â”‚   â”‚   â”œâ”€â”€ experimental/    # Experimental model implementations
â”‚   â”‚   â”‚   â””â”€â”€ statistical/     # Statistical forecasting models
â”‚   â”‚   â””â”€â”€ services/            # Domain services
â”‚   â”œâ”€â”€ infrastructure/          # Infrastructure layer
â”‚   â”‚   â”œâ”€â”€ results/             # Results storage and management
â”‚   â”‚   â”œâ”€â”€ storage/             # Data storage components
â”‚   â”‚   â””â”€â”€ ui/                  # UI integration components
â”‚   â”œâ”€â”€ tests/                   # Test suite
â”‚   â””â”€â”€ utils/                   # Utility functions
â”œâ”€â”€ frontend/                    # React frontend (planned)
â”‚   â”œâ”€â”€ public/                  # Static assets
â”‚   â””â”€â”€ src/                     # Source code
â”‚       â”œâ”€â”€ assets/              # Frontend assets
â”‚       â””â”€â”€ components/          # React components
â”œâ”€â”€ Images/                      # Project images
â”œâ”€â”€ k8s/                         # Kubernetes configuration
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ Anomaly detection/       # Anomaly detection experiments
â”‚   â”œâ”€â”€ Darts/                   # Darts library experiments
â”‚   â”œâ”€â”€ m3/                      # M3 competition datasets
â”‚   â”œâ”€â”€ Other/                   # Miscellaneous notebooks
â”‚   â””â”€â”€ preprocessing/           # Data preprocessing experiments
â””â”€â”€ darts_logs/                  # Model training logs
```

The project follows a clean architecture approach with distinct layers:

- **API Layer**: Handles HTTP requests and responses
- **Application Layer**: Orchestrates use cases and workflows
- **Domain Layer**: Contains business logic and model implementations
- **Infrastructure Layer**: Provides implementation details for external interactions

## Contributing

We welcome contributions! To contribute:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for details on code style, testing requirements, and process.

## License

This project is licensed under the terms of the [LICENSE](LICENSE) file.

## Contact

For any inquiries, please open an issue in the GitHub repository.
